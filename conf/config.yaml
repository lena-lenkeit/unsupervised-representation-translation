defaults:
  - model: pythia40m
  - dataset: causal
  - trainer: causal
  - _self_

training:
  output_dir: ./results/pythia-70m-alt-dropout
  num_train_epochs: 1
  per_device_train_batch_size: 16
  warmup_steps: 0
  weight_decay: 0.0
  logging_dir: ./logs
  logging_steps: 9
  save_total_limit: 1
  learning_rate: 5e-5
  lr_scheduler_type: constant
  remove_unused_columns: False
  gradient_checkpointing: True
  optim: adamw_bnb_8bit

eval:
  do_clm: false

hydra:
  run:
    dir: .